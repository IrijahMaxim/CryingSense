# Performance Reports

This directory contains comprehensive model performance reports.

## Contents
- Classification metrics (precision, recall, F1-score)
- Per-class performance breakdowns
- ROC curves and AUC scores
- Inference latency measurements
- Model size and memory usage statistics

## Report Format
Reports are saved in both JSON and Markdown formats:
- `performance_report_{model}_{date}.json` - Machine-readable
- `performance_report_{model}_{date}.md` - Human-readable

## Metrics Tracked

### Classification Metrics
- Overall accuracy
- Per-class precision, recall, F1-score
- Macro/micro averages
- Confusion matrix

### Inference Performance
- Average inference time
- 95th percentile latency
- Throughput (samples/second)
- Memory usage

### Model Characteristics
- Number of parameters
- Model size (MB)
- FLOPs per inference
- Quantization impact

## Generation
Reports are automatically generated by:
```bash
python model/training/evaluate.py --model path/to/model.pth --test-data path/to/test/data
```

## Comparison
Use comparison tools to track improvements across experiments:
```bash
python scripts/compare_models.py --reports experiments/performance_reports/*.json
```
